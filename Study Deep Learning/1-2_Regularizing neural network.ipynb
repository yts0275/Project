{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting / High Variance 의 경우\n",
    "\n",
    "1. 정규화를 시도한다.  \n",
    "2. 신뢰성이 높은 데이터를 더 수집한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regualrization 방법\n",
    "#### Logistic regression case\n",
    "#### Neural network case\n",
    "\n",
    " - regualrization 의 의미\n",
    " - weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 - Regualrization\n",
    "### Regualrization 이 편차를 줄이는 이유 - 1\n",
    "\n",
    "직관적으로 Lambda 일반화를 크게 정하면, weight matrix인 W의 값을 0에 가까운 값으로 지정하고 싶어할 것이다.  \n",
    " = 수 많은 숨겨진 유닛에 대해 weight을 거의 0에 가까운 값으로 지정하여 숨겨진 유닛의 영향을 거의 0으로 만들어 버리는 것   \n",
    "이런 경우,로지스틱 회귀 모형과 같이 단순해진다. -> 큰 편차를 갖는 경우에서 큰 편향을 갖는 경우로 이동된다.  \n",
    "이상적으론 적당한 중간 값의 Lambda를 통해 적절한 신경망의 케이스가 나올 수도 있다.  \n",
    "  \n",
    " - Lambda 의 값이 크다면, 신경망을 조금 더 단순하게 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regualrization 이 편차를 줄이는 이유 - 2\n",
    "\n",
    "강의의 예제에선 tan h activation 함수를 이용한다고 가정한다.\n",
    "  - g(z) = tanh(z)\n",
    "  \n",
    "1. z가 작은 범위의 파라미터 값만 갖는다면 단순히 tanh 함수의 선형적인 부분을 사용하는 것\n",
    "2. z가 큰 값이나 작은 값까지 이동할 수 있는 경우에만 activation 함수가 덜 선형적인 함수로 정의됨\n",
    "    \n",
    "#### Lambda의 값 ( 일반화 파라미터의 값 ) 이 큰 경우\n",
    "    - 파라미터들이 꽤 작을 것\n",
    "    - Z = Wa + b\n",
    "        - weight 인 W 가 작은 경우 Z 또한 작을 것이다.\n",
    "    - 이럴 경우 G(z) 가 대략적으로 선형적인 함수를 이룰 것이다. ( 1번 참조 )\n",
    "    - activation function 이 선형적일 경우 deep 한 network 가 의미가 없다 = 복잡한 결정이 불가능하다.\n",
    "\n",
    "#### 요약\n",
    "일반화가 매우 커지는 경우,  \n",
    "W 매개 변수가 매우 작아지고,  \n",
    "그로인해 Z도 상대적으로 작은 범위의 값을 가질 것이다.  \n",
    "activation 함수의 모양에 따라 작은 범위의 값을 가질때의 모양새를 가지게 되고,  \n",
    "만약 tanh 처럼 z 가 작은 범위일 때 선형적인 함수를 가진다면,  \n",
    "전체적인 네트워크도 선형함수를 가지게 되고,  \n",
    "복잡한 네트워크의 기능을 못하고, 비선형적인 결정을 못하기 때문에,  \n",
    "overfitting이 덜해진다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regulazation\n",
    "\n",
    "네트워크의 각각 층별로 살펴보면서 신경망의 노드를 제거하는 확률을 세팅  \n",
    "확률에 따라 노드를 제거하며, 들어오고 나가는 링크 또한 같이 제거  \n",
    "확률에 따라 훨씬 더 작은, 감소된 네트워크가 남을 것  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout 은 실제로 쓰지 않을 예상! 여기까지만 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
